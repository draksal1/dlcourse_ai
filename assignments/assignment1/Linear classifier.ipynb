{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(float)\n",
    "target_index = np.ones(batch_size, dtype=int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397363\n",
      "Epoch 1, loss: 2.330354\n",
      "Epoch 2, loss: 2.311002\n",
      "Epoch 3, loss: 2.303897\n",
      "Epoch 4, loss: 2.303257\n",
      "Epoch 5, loss: 2.302898\n",
      "Epoch 6, loss: 2.302564\n",
      "Epoch 7, loss: 2.301815\n",
      "Epoch 8, loss: 2.301252\n",
      "Epoch 9, loss: 2.301256\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2516a2c43d0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.127\n",
      "Epoch 0, loss: 2.301800\n",
      "Epoch 1, loss: 2.302558\n",
      "Epoch 2, loss: 2.302208\n",
      "Epoch 3, loss: 2.303309\n",
      "Epoch 4, loss: 2.302126\n",
      "Epoch 5, loss: 2.301524\n",
      "Epoch 6, loss: 2.301150\n",
      "Epoch 7, loss: 2.301359\n",
      "Epoch 8, loss: 2.302305\n",
      "Epoch 9, loss: 2.302205\n",
      "Epoch 10, loss: 2.301595\n",
      "Epoch 11, loss: 2.302039\n",
      "Epoch 12, loss: 2.301247\n",
      "Epoch 13, loss: 2.301836\n",
      "Epoch 14, loss: 2.302396\n",
      "Epoch 15, loss: 2.302299\n",
      "Epoch 16, loss: 2.301161\n",
      "Epoch 17, loss: 2.302058\n",
      "Epoch 18, loss: 2.302180\n",
      "Epoch 19, loss: 2.302339\n",
      "Epoch 20, loss: 2.302694\n",
      "Epoch 21, loss: 2.301872\n",
      "Epoch 22, loss: 2.302261\n",
      "Epoch 23, loss: 2.302209\n",
      "Epoch 24, loss: 2.302215\n",
      "Epoch 25, loss: 2.301591\n",
      "Epoch 26, loss: 2.301535\n",
      "Epoch 27, loss: 2.302557\n",
      "Epoch 28, loss: 2.301689\n",
      "Epoch 29, loss: 2.301815\n",
      "Epoch 30, loss: 2.301483\n",
      "Epoch 31, loss: 2.301553\n",
      "Epoch 32, loss: 2.301640\n",
      "Epoch 33, loss: 2.301991\n",
      "Epoch 34, loss: 2.302104\n",
      "Epoch 35, loss: 2.301741\n",
      "Epoch 36, loss: 2.302652\n",
      "Epoch 37, loss: 2.303017\n",
      "Epoch 38, loss: 2.301521\n",
      "Epoch 39, loss: 2.301504\n",
      "Epoch 40, loss: 2.301565\n",
      "Epoch 41, loss: 2.302361\n",
      "Epoch 42, loss: 2.302223\n",
      "Epoch 43, loss: 2.302385\n",
      "Epoch 44, loss: 2.302532\n",
      "Epoch 45, loss: 2.301950\n",
      "Epoch 46, loss: 2.301686\n",
      "Epoch 47, loss: 2.301881\n",
      "Epoch 48, loss: 2.302291\n",
      "Epoch 49, loss: 2.302069\n",
      "Epoch 50, loss: 2.301971\n",
      "Epoch 51, loss: 2.302533\n",
      "Epoch 52, loss: 2.302043\n",
      "Epoch 53, loss: 2.302003\n",
      "Epoch 54, loss: 2.301729\n",
      "Epoch 55, loss: 2.301272\n",
      "Epoch 56, loss: 2.302220\n",
      "Epoch 57, loss: 2.301765\n",
      "Epoch 58, loss: 2.301959\n",
      "Epoch 59, loss: 2.302229\n",
      "Epoch 60, loss: 2.301372\n",
      "Epoch 61, loss: 2.302540\n",
      "Epoch 62, loss: 2.301929\n",
      "Epoch 63, loss: 2.301502\n",
      "Epoch 64, loss: 2.302201\n",
      "Epoch 65, loss: 2.302674\n",
      "Epoch 66, loss: 2.302191\n",
      "Epoch 67, loss: 2.302989\n",
      "Epoch 68, loss: 2.301388\n",
      "Epoch 69, loss: 2.301621\n",
      "Epoch 70, loss: 2.301533\n",
      "Epoch 71, loss: 2.302804\n",
      "Epoch 72, loss: 2.301514\n",
      "Epoch 73, loss: 2.300962\n",
      "Epoch 74, loss: 2.302366\n",
      "Epoch 75, loss: 2.301895\n",
      "Epoch 76, loss: 2.301768\n",
      "Epoch 77, loss: 2.302649\n",
      "Epoch 78, loss: 2.302394\n",
      "Epoch 79, loss: 2.301415\n",
      "Epoch 80, loss: 2.302148\n",
      "Epoch 81, loss: 2.302263\n",
      "Epoch 82, loss: 2.301172\n",
      "Epoch 83, loss: 2.302803\n",
      "Epoch 84, loss: 2.301892\n",
      "Epoch 85, loss: 2.302788\n",
      "Epoch 86, loss: 2.302016\n",
      "Epoch 87, loss: 2.302450\n",
      "Epoch 88, loss: 2.302002\n",
      "Epoch 89, loss: 2.302363\n",
      "Epoch 90, loss: 2.301257\n",
      "Epoch 91, loss: 2.302553\n",
      "Epoch 92, loss: 2.301951\n",
      "Epoch 93, loss: 2.300910\n",
      "Epoch 94, loss: 2.302318\n",
      "Epoch 95, loss: 2.301563\n",
      "Epoch 96, loss: 2.302494\n",
      "Epoch 97, loss: 2.301579\n",
      "Epoch 98, loss: 2.302468\n",
      "Epoch 99, loss: 2.302352\n",
      "Accuracy after training for 100 epochs:  0.121\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.301738\n",
      "Epoch 1, loss: 2.300634\n",
      "Epoch 2, loss: 2.300803\n",
      "Epoch 3, loss: 2.298726\n",
      "Epoch 4, loss: 2.296476\n",
      "Epoch 5, loss: 2.295691\n",
      "Epoch 6, loss: 2.291721\n",
      "Epoch 7, loss: 2.291423\n",
      "Epoch 8, loss: 2.296155\n",
      "Epoch 9, loss: 2.292435\n",
      "Epoch 10, loss: 2.290490\n",
      "Epoch 11, loss: 2.292991\n",
      "Epoch 12, loss: 2.293113\n",
      "Epoch 13, loss: 2.295678\n",
      "Epoch 14, loss: 2.284954\n",
      "Epoch 15, loss: 2.292666\n",
      "Epoch 16, loss: 2.285464\n",
      "Epoch 17, loss: 2.288212\n",
      "Epoch 18, loss: 2.289307\n",
      "Epoch 19, loss: 2.286572\n",
      "Epoch 20, loss: 2.279525\n",
      "Epoch 21, loss: 2.286214\n",
      "Epoch 22, loss: 2.283703\n",
      "Epoch 23, loss: 2.283704\n",
      "Epoch 24, loss: 2.287414\n",
      "Epoch 25, loss: 2.282707\n",
      "Epoch 26, loss: 2.280525\n",
      "Epoch 27, loss: 2.271682\n",
      "Epoch 28, loss: 2.281189\n",
      "Epoch 29, loss: 2.273233\n",
      "Epoch 30, loss: 2.267395\n",
      "Epoch 31, loss: 2.278831\n",
      "Epoch 32, loss: 2.283337\n",
      "Epoch 33, loss: 2.267443\n",
      "Epoch 34, loss: 2.270539\n",
      "Epoch 35, loss: 2.271762\n",
      "Epoch 36, loss: 2.261442\n",
      "Epoch 37, loss: 2.275850\n",
      "Epoch 38, loss: 2.264095\n",
      "Epoch 39, loss: 2.272777\n",
      "Epoch 40, loss: 2.260102\n",
      "Epoch 41, loss: 2.276132\n",
      "Epoch 42, loss: 2.272784\n",
      "Epoch 43, loss: 2.262592\n",
      "Epoch 44, loss: 2.272305\n",
      "Epoch 45, loss: 2.270637\n",
      "Epoch 46, loss: 2.269510\n",
      "Epoch 47, loss: 2.272183\n",
      "Epoch 48, loss: 2.260136\n",
      "Epoch 49, loss: 2.269049\n",
      "Epoch 50, loss: 2.250925\n",
      "Epoch 51, loss: 2.257433\n",
      "Epoch 52, loss: 2.253868\n",
      "Epoch 53, loss: 2.271120\n",
      "Epoch 54, loss: 2.249027\n",
      "Epoch 55, loss: 2.257061\n",
      "Epoch 56, loss: 2.271050\n",
      "Epoch 57, loss: 2.254839\n",
      "Epoch 58, loss: 2.264249\n",
      "Epoch 59, loss: 2.256546\n",
      "Epoch 60, loss: 2.265193\n",
      "Epoch 61, loss: 2.249265\n",
      "Epoch 62, loss: 2.251350\n",
      "Epoch 63, loss: 2.263895\n",
      "Epoch 64, loss: 2.251979\n",
      "Epoch 65, loss: 2.267844\n",
      "Epoch 66, loss: 2.250120\n",
      "Epoch 67, loss: 2.259770\n",
      "Epoch 68, loss: 2.268374\n",
      "Epoch 69, loss: 2.246070\n",
      "Epoch 70, loss: 2.254479\n",
      "Epoch 71, loss: 2.253845\n",
      "Epoch 72, loss: 2.234930\n",
      "Epoch 73, loss: 2.253486\n",
      "Epoch 74, loss: 2.260858\n",
      "Epoch 75, loss: 2.246499\n",
      "Epoch 76, loss: 2.257934\n",
      "Epoch 77, loss: 2.243347\n",
      "Epoch 78, loss: 2.231033\n",
      "Epoch 79, loss: 2.248701\n",
      "Epoch 80, loss: 2.246558\n",
      "Epoch 81, loss: 2.247641\n",
      "Epoch 82, loss: 2.247629\n",
      "Epoch 83, loss: 2.241413\n",
      "Epoch 84, loss: 2.257257\n",
      "Epoch 85, loss: 2.237919\n",
      "Epoch 86, loss: 2.245560\n",
      "Epoch 87, loss: 2.252838\n",
      "Epoch 88, loss: 2.245258\n",
      "Epoch 89, loss: 2.248739\n",
      "Epoch 90, loss: 2.225559\n",
      "Epoch 91, loss: 2.246867\n",
      "Epoch 92, loss: 2.225856\n",
      "Epoch 93, loss: 2.250530\n",
      "Epoch 94, loss: 2.241552\n",
      "Epoch 95, loss: 2.241519\n",
      "Epoch 96, loss: 2.235963\n",
      "Epoch 97, loss: 2.230151\n",
      "Epoch 98, loss: 2.237276\n",
      "Epoch 99, loss: 2.259893\n",
      "Epoch 100, loss: 2.251293\n",
      "Epoch 101, loss: 2.244830\n",
      "Epoch 102, loss: 2.237166\n",
      "Epoch 103, loss: 2.241804\n",
      "Epoch 104, loss: 2.256727\n",
      "Epoch 105, loss: 2.235447\n",
      "Epoch 106, loss: 2.211972\n",
      "Epoch 107, loss: 2.219134\n",
      "Epoch 108, loss: 2.238348\n",
      "Epoch 109, loss: 2.229007\n",
      "Epoch 110, loss: 2.237743\n",
      "Epoch 111, loss: 2.230696\n",
      "Epoch 112, loss: 2.248126\n",
      "Epoch 113, loss: 2.233632\n",
      "Epoch 114, loss: 2.238296\n",
      "Epoch 115, loss: 2.238922\n",
      "Epoch 116, loss: 2.218621\n",
      "Epoch 117, loss: 2.235935\n",
      "Epoch 118, loss: 2.239624\n",
      "Epoch 119, loss: 2.254898\n",
      "Epoch 120, loss: 2.231786\n",
      "Epoch 121, loss: 2.231345\n",
      "Epoch 122, loss: 2.233690\n",
      "Epoch 123, loss: 2.234395\n",
      "Epoch 124, loss: 2.213405\n",
      "Epoch 125, loss: 2.239463\n",
      "Epoch 126, loss: 2.207448\n",
      "Epoch 127, loss: 2.205509\n",
      "Epoch 128, loss: 2.220195\n",
      "Epoch 129, loss: 2.232427\n",
      "Epoch 130, loss: 2.229680\n",
      "Epoch 131, loss: 2.245710\n",
      "Epoch 132, loss: 2.232215\n",
      "Epoch 133, loss: 2.212125\n",
      "Epoch 134, loss: 2.214090\n",
      "Epoch 135, loss: 2.219644\n",
      "Epoch 136, loss: 2.213746\n",
      "Epoch 137, loss: 2.231117\n",
      "Epoch 138, loss: 2.214592\n",
      "Epoch 139, loss: 2.227238\n",
      "Epoch 140, loss: 2.222889\n",
      "Epoch 141, loss: 2.214735\n",
      "Epoch 142, loss: 2.229886\n",
      "Epoch 143, loss: 2.227929\n",
      "Epoch 144, loss: 2.200242\n",
      "Epoch 145, loss: 2.223495\n",
      "Epoch 146, loss: 2.205658\n",
      "Epoch 147, loss: 2.230145\n",
      "Epoch 148, loss: 2.206939\n",
      "Epoch 149, loss: 2.225365\n",
      "Epoch 150, loss: 2.204039\n",
      "Epoch 151, loss: 2.230950\n",
      "Epoch 152, loss: 2.213716\n",
      "Epoch 153, loss: 2.213648\n",
      "Epoch 154, loss: 2.226757\n",
      "Epoch 155, loss: 2.195406\n",
      "Epoch 156, loss: 2.212731\n",
      "Epoch 157, loss: 2.212640\n",
      "Epoch 158, loss: 2.225788\n",
      "Epoch 159, loss: 2.200660\n",
      "Epoch 160, loss: 2.227523\n",
      "Epoch 161, loss: 2.210939\n",
      "Epoch 162, loss: 2.228270\n",
      "Epoch 163, loss: 2.207888\n",
      "Epoch 164, loss: 2.207016\n",
      "Epoch 165, loss: 2.206839\n",
      "Epoch 166, loss: 2.193466\n",
      "Epoch 167, loss: 2.206968\n",
      "Epoch 168, loss: 2.234777\n",
      "Epoch 169, loss: 2.198879\n",
      "Epoch 170, loss: 2.194620\n",
      "Epoch 171, loss: 2.208572\n",
      "Epoch 172, loss: 2.222942\n",
      "Epoch 173, loss: 2.203284\n",
      "Epoch 174, loss: 2.213971\n",
      "Epoch 175, loss: 2.223064\n",
      "Epoch 176, loss: 2.201779\n",
      "Epoch 177, loss: 2.220306\n",
      "Epoch 178, loss: 2.221013\n",
      "Epoch 179, loss: 2.197742\n",
      "Epoch 180, loss: 2.209498\n",
      "Epoch 181, loss: 2.199228\n",
      "Epoch 182, loss: 2.214471\n",
      "Epoch 183, loss: 2.225021\n",
      "Epoch 184, loss: 2.219538\n",
      "Epoch 185, loss: 2.227156\n",
      "Epoch 186, loss: 2.211218\n",
      "Epoch 187, loss: 2.193969\n",
      "Epoch 188, loss: 2.206194\n",
      "Epoch 189, loss: 2.183236\n",
      "Epoch 190, loss: 2.196535\n",
      "Epoch 191, loss: 2.186173\n",
      "Epoch 192, loss: 2.192144\n",
      "Epoch 193, loss: 2.203718\n",
      "Epoch 194, loss: 2.201476\n",
      "Epoch 195, loss: 2.215047\n",
      "Epoch 196, loss: 2.192003\n",
      "Epoch 197, loss: 2.217356\n",
      "Epoch 198, loss: 2.184553\n",
      "Epoch 199, loss: 2.189460\n",
      "Epoch 0, loss: 2.302129\n",
      "Epoch 1, loss: 2.300424\n",
      "Epoch 2, loss: 2.297668\n",
      "Epoch 3, loss: 2.298296\n",
      "Epoch 4, loss: 2.297825\n",
      "Epoch 5, loss: 2.296578\n",
      "Epoch 6, loss: 2.293819\n",
      "Epoch 7, loss: 2.294970\n",
      "Epoch 8, loss: 2.294033\n",
      "Epoch 9, loss: 2.292382\n",
      "Epoch 10, loss: 2.287598\n",
      "Epoch 11, loss: 2.294209\n",
      "Epoch 12, loss: 2.293367\n",
      "Epoch 13, loss: 2.291295\n",
      "Epoch 14, loss: 2.286368\n",
      "Epoch 15, loss: 2.291077\n",
      "Epoch 16, loss: 2.285753\n",
      "Epoch 17, loss: 2.290092\n",
      "Epoch 18, loss: 2.286510\n",
      "Epoch 19, loss: 2.282915\n",
      "Epoch 20, loss: 2.287177\n",
      "Epoch 21, loss: 2.287994\n",
      "Epoch 22, loss: 2.279104\n",
      "Epoch 23, loss: 2.283140\n",
      "Epoch 24, loss: 2.280689\n",
      "Epoch 25, loss: 2.282042\n",
      "Epoch 26, loss: 2.281763\n",
      "Epoch 27, loss: 2.278218\n",
      "Epoch 28, loss: 2.284177\n",
      "Epoch 29, loss: 2.275854\n",
      "Epoch 30, loss: 2.280279\n",
      "Epoch 31, loss: 2.274020\n",
      "Epoch 32, loss: 2.272898\n",
      "Epoch 33, loss: 2.274113\n",
      "Epoch 34, loss: 2.282381\n",
      "Epoch 35, loss: 2.270542\n",
      "Epoch 36, loss: 2.277494\n",
      "Epoch 37, loss: 2.265592\n",
      "Epoch 38, loss: 2.273610\n",
      "Epoch 39, loss: 2.263385\n",
      "Epoch 40, loss: 2.263826\n",
      "Epoch 41, loss: 2.273295\n",
      "Epoch 42, loss: 2.265442\n",
      "Epoch 43, loss: 2.271573\n",
      "Epoch 44, loss: 2.267403\n",
      "Epoch 45, loss: 2.276081\n",
      "Epoch 46, loss: 2.263683\n",
      "Epoch 47, loss: 2.264506\n",
      "Epoch 48, loss: 2.266760\n",
      "Epoch 49, loss: 2.264161\n",
      "Epoch 50, loss: 2.262616\n",
      "Epoch 51, loss: 2.265245\n",
      "Epoch 52, loss: 2.256830\n",
      "Epoch 53, loss: 2.268529\n",
      "Epoch 54, loss: 2.263167\n",
      "Epoch 55, loss: 2.257294\n",
      "Epoch 56, loss: 2.271367\n",
      "Epoch 57, loss: 2.249135\n",
      "Epoch 58, loss: 2.255808\n",
      "Epoch 59, loss: 2.257143\n",
      "Epoch 60, loss: 2.261401\n",
      "Epoch 61, loss: 2.254207\n",
      "Epoch 62, loss: 2.258711\n",
      "Epoch 63, loss: 2.251831\n",
      "Epoch 64, loss: 2.246887\n",
      "Epoch 65, loss: 2.257864\n",
      "Epoch 66, loss: 2.253256\n",
      "Epoch 67, loss: 2.265299\n",
      "Epoch 68, loss: 2.247198\n",
      "Epoch 69, loss: 2.254782\n",
      "Epoch 70, loss: 2.244641\n",
      "Epoch 71, loss: 2.248512\n",
      "Epoch 72, loss: 2.264741\n",
      "Epoch 73, loss: 2.260719\n",
      "Epoch 74, loss: 2.256526\n",
      "Epoch 75, loss: 2.248263\n",
      "Epoch 76, loss: 2.264451\n",
      "Epoch 77, loss: 2.244956\n",
      "Epoch 78, loss: 2.256515\n",
      "Epoch 79, loss: 2.247581\n",
      "Epoch 80, loss: 2.247928\n",
      "Epoch 81, loss: 2.237212\n",
      "Epoch 82, loss: 2.241998\n",
      "Epoch 83, loss: 2.258773\n",
      "Epoch 84, loss: 2.232272\n",
      "Epoch 85, loss: 2.250793\n",
      "Epoch 86, loss: 2.236055\n",
      "Epoch 87, loss: 2.260374\n",
      "Epoch 88, loss: 2.240495\n",
      "Epoch 89, loss: 2.237555\n",
      "Epoch 90, loss: 2.236870\n",
      "Epoch 91, loss: 2.240263\n",
      "Epoch 92, loss: 2.234261\n",
      "Epoch 93, loss: 2.246006\n",
      "Epoch 94, loss: 2.246061\n",
      "Epoch 95, loss: 2.238979\n",
      "Epoch 96, loss: 2.253237\n",
      "Epoch 97, loss: 2.241123\n",
      "Epoch 98, loss: 2.233405\n",
      "Epoch 99, loss: 2.229992\n",
      "Epoch 100, loss: 2.232679\n",
      "Epoch 101, loss: 2.241489\n",
      "Epoch 102, loss: 2.222919\n",
      "Epoch 103, loss: 2.238423\n",
      "Epoch 104, loss: 2.231417\n",
      "Epoch 105, loss: 2.258207\n",
      "Epoch 106, loss: 2.248251\n",
      "Epoch 107, loss: 2.228924\n",
      "Epoch 108, loss: 2.231655\n",
      "Epoch 109, loss: 2.225887\n",
      "Epoch 110, loss: 2.233268\n",
      "Epoch 111, loss: 2.235825\n",
      "Epoch 112, loss: 2.211097\n",
      "Epoch 113, loss: 2.220769\n",
      "Epoch 114, loss: 2.232350\n",
      "Epoch 115, loss: 2.220717\n",
      "Epoch 116, loss: 2.238497\n",
      "Epoch 117, loss: 2.211016\n",
      "Epoch 118, loss: 2.222709\n",
      "Epoch 119, loss: 2.247162\n",
      "Epoch 120, loss: 2.227734\n",
      "Epoch 121, loss: 2.231873\n",
      "Epoch 122, loss: 2.240496\n",
      "Epoch 123, loss: 2.227850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124, loss: 2.239292\n",
      "Epoch 125, loss: 2.233647\n",
      "Epoch 126, loss: 2.229735\n",
      "Epoch 127, loss: 2.232487\n",
      "Epoch 128, loss: 2.200344\n",
      "Epoch 129, loss: 2.245746\n",
      "Epoch 130, loss: 2.231853\n",
      "Epoch 131, loss: 2.228736\n",
      "Epoch 132, loss: 2.219033\n",
      "Epoch 133, loss: 2.229271\n",
      "Epoch 134, loss: 2.212380\n",
      "Epoch 135, loss: 2.228612\n",
      "Epoch 136, loss: 2.214206\n",
      "Epoch 137, loss: 2.200207\n",
      "Epoch 138, loss: 2.212405\n",
      "Epoch 139, loss: 2.214716\n",
      "Epoch 140, loss: 2.226104\n",
      "Epoch 141, loss: 2.216618\n",
      "Epoch 142, loss: 2.233583\n",
      "Epoch 143, loss: 2.216062\n",
      "Epoch 144, loss: 2.220387\n",
      "Epoch 145, loss: 2.252343\n",
      "Epoch 146, loss: 2.234937\n",
      "Epoch 147, loss: 2.212437\n",
      "Epoch 148, loss: 2.205648\n",
      "Epoch 149, loss: 2.215865\n",
      "Epoch 150, loss: 2.230772\n",
      "Epoch 151, loss: 2.220912\n",
      "Epoch 152, loss: 2.240325\n",
      "Epoch 153, loss: 2.213539\n",
      "Epoch 154, loss: 2.185198\n",
      "Epoch 155, loss: 2.208413\n",
      "Epoch 156, loss: 2.215578\n",
      "Epoch 157, loss: 2.233672\n",
      "Epoch 158, loss: 2.228951\n",
      "Epoch 159, loss: 2.224026\n",
      "Epoch 160, loss: 2.200218\n",
      "Epoch 161, loss: 2.211529\n",
      "Epoch 162, loss: 2.220934\n",
      "Epoch 163, loss: 2.180421\n",
      "Epoch 164, loss: 2.183895\n",
      "Epoch 165, loss: 2.215818\n",
      "Epoch 166, loss: 2.187779\n",
      "Epoch 167, loss: 2.216317\n",
      "Epoch 168, loss: 2.223224\n",
      "Epoch 169, loss: 2.213814\n",
      "Epoch 170, loss: 2.204604\n",
      "Epoch 171, loss: 2.207806\n",
      "Epoch 172, loss: 2.212280\n",
      "Epoch 173, loss: 2.190172\n",
      "Epoch 174, loss: 2.213691\n",
      "Epoch 175, loss: 2.217293\n",
      "Epoch 176, loss: 2.175582\n",
      "Epoch 177, loss: 2.190137\n",
      "Epoch 178, loss: 2.215722\n",
      "Epoch 179, loss: 2.199241\n",
      "Epoch 180, loss: 2.213463\n",
      "Epoch 181, loss: 2.198485\n",
      "Epoch 182, loss: 2.226422\n",
      "Epoch 183, loss: 2.189654\n",
      "Epoch 184, loss: 2.203428\n",
      "Epoch 185, loss: 2.200886\n",
      "Epoch 186, loss: 2.205892\n",
      "Epoch 187, loss: 2.173915\n",
      "Epoch 188, loss: 2.161004\n",
      "Epoch 189, loss: 2.201265\n",
      "Epoch 190, loss: 2.220705\n",
      "Epoch 191, loss: 2.185592\n",
      "Epoch 192, loss: 2.202780\n",
      "Epoch 193, loss: 2.217686\n",
      "Epoch 194, loss: 2.207156\n",
      "Epoch 195, loss: 2.215332\n",
      "Epoch 196, loss: 2.206577\n",
      "Epoch 197, loss: 2.214496\n",
      "Epoch 198, loss: 2.211839\n",
      "Epoch 199, loss: 2.191949\n",
      "Epoch 0, loss: 2.301249\n",
      "Epoch 1, loss: 2.303465\n",
      "Epoch 2, loss: 2.300686\n",
      "Epoch 3, loss: 2.299152\n",
      "Epoch 4, loss: 2.299444\n",
      "Epoch 5, loss: 2.297381\n",
      "Epoch 6, loss: 2.293227\n",
      "Epoch 7, loss: 2.295024\n",
      "Epoch 8, loss: 2.298694\n",
      "Epoch 9, loss: 2.292457\n",
      "Epoch 10, loss: 2.291555\n",
      "Epoch 11, loss: 2.292836\n",
      "Epoch 12, loss: 2.287972\n",
      "Epoch 13, loss: 2.289647\n",
      "Epoch 14, loss: 2.287617\n",
      "Epoch 15, loss: 2.287926\n",
      "Epoch 16, loss: 2.289581\n",
      "Epoch 17, loss: 2.285662\n",
      "Epoch 18, loss: 2.286178\n",
      "Epoch 19, loss: 2.287985\n",
      "Epoch 20, loss: 2.283605\n",
      "Epoch 21, loss: 2.286220\n",
      "Epoch 22, loss: 2.283305\n",
      "Epoch 23, loss: 2.284177\n",
      "Epoch 24, loss: 2.283504\n",
      "Epoch 25, loss: 2.277691\n",
      "Epoch 26, loss: 2.279506\n",
      "Epoch 27, loss: 2.282218\n",
      "Epoch 28, loss: 2.276776\n",
      "Epoch 29, loss: 2.286433\n",
      "Epoch 30, loss: 2.277271\n",
      "Epoch 31, loss: 2.281774\n",
      "Epoch 32, loss: 2.276455\n",
      "Epoch 33, loss: 2.279447\n",
      "Epoch 34, loss: 2.273556\n",
      "Epoch 35, loss: 2.276353\n",
      "Epoch 36, loss: 2.269425\n",
      "Epoch 37, loss: 2.275032\n",
      "Epoch 38, loss: 2.271484\n",
      "Epoch 39, loss: 2.259728\n",
      "Epoch 40, loss: 2.271680\n",
      "Epoch 41, loss: 2.279747\n",
      "Epoch 42, loss: 2.266696\n",
      "Epoch 43, loss: 2.270861\n",
      "Epoch 44, loss: 2.267887\n",
      "Epoch 45, loss: 2.272040\n",
      "Epoch 46, loss: 2.270030\n",
      "Epoch 47, loss: 2.265264\n",
      "Epoch 48, loss: 2.261388\n",
      "Epoch 49, loss: 2.263552\n",
      "Epoch 50, loss: 2.261854\n",
      "Epoch 51, loss: 2.258131\n",
      "Epoch 52, loss: 2.268728\n",
      "Epoch 53, loss: 2.276193\n",
      "Epoch 54, loss: 2.262879\n",
      "Epoch 55, loss: 2.262798\n",
      "Epoch 56, loss: 2.265207\n",
      "Epoch 57, loss: 2.250923\n",
      "Epoch 58, loss: 2.249118\n",
      "Epoch 59, loss: 2.266702\n",
      "Epoch 60, loss: 2.257006\n",
      "Epoch 61, loss: 2.257396\n",
      "Epoch 62, loss: 2.261110\n",
      "Epoch 63, loss: 2.255925\n",
      "Epoch 64, loss: 2.260785\n",
      "Epoch 65, loss: 2.257805\n",
      "Epoch 66, loss: 2.253570\n",
      "Epoch 67, loss: 2.242455\n",
      "Epoch 68, loss: 2.269270\n",
      "Epoch 69, loss: 2.245397\n",
      "Epoch 70, loss: 2.248046\n",
      "Epoch 71, loss: 2.240185\n",
      "Epoch 72, loss: 2.255449\n",
      "Epoch 73, loss: 2.244411\n",
      "Epoch 74, loss: 2.248826\n",
      "Epoch 75, loss: 2.262489\n",
      "Epoch 76, loss: 2.242825\n",
      "Epoch 77, loss: 2.257435\n",
      "Epoch 78, loss: 2.251269\n",
      "Epoch 79, loss: 2.242801\n",
      "Epoch 80, loss: 2.249189\n",
      "Epoch 81, loss: 2.242127\n",
      "Epoch 82, loss: 2.244213\n",
      "Epoch 83, loss: 2.253255\n",
      "Epoch 84, loss: 2.244788\n",
      "Epoch 85, loss: 2.234755\n",
      "Epoch 86, loss: 2.244777\n",
      "Epoch 87, loss: 2.232543\n",
      "Epoch 88, loss: 2.234265\n",
      "Epoch 89, loss: 2.234471\n",
      "Epoch 90, loss: 2.239876\n",
      "Epoch 91, loss: 2.250342\n",
      "Epoch 92, loss: 2.253518\n",
      "Epoch 93, loss: 2.230098\n",
      "Epoch 94, loss: 2.232790\n",
      "Epoch 95, loss: 2.256126\n",
      "Epoch 96, loss: 2.242688\n",
      "Epoch 97, loss: 2.246461\n",
      "Epoch 98, loss: 2.230111\n",
      "Epoch 99, loss: 2.239481\n",
      "Epoch 100, loss: 2.246303\n",
      "Epoch 101, loss: 2.226974\n",
      "Epoch 102, loss: 2.253024\n",
      "Epoch 103, loss: 2.237750\n",
      "Epoch 104, loss: 2.251266\n",
      "Epoch 105, loss: 2.236447\n",
      "Epoch 106, loss: 2.211015\n",
      "Epoch 107, loss: 2.228308\n",
      "Epoch 108, loss: 2.240159\n",
      "Epoch 109, loss: 2.236014\n",
      "Epoch 110, loss: 2.210126\n",
      "Epoch 111, loss: 2.241088\n",
      "Epoch 112, loss: 2.227205\n",
      "Epoch 113, loss: 2.229717\n",
      "Epoch 114, loss: 2.233707\n",
      "Epoch 115, loss: 2.238913\n",
      "Epoch 116, loss: 2.239989\n",
      "Epoch 117, loss: 2.217122\n",
      "Epoch 118, loss: 2.226515\n",
      "Epoch 119, loss: 2.220189\n",
      "Epoch 120, loss: 2.225621\n",
      "Epoch 121, loss: 2.229031\n",
      "Epoch 122, loss: 2.229941\n",
      "Epoch 123, loss: 2.249683\n",
      "Epoch 124, loss: 2.236309\n",
      "Epoch 125, loss: 2.223468\n",
      "Epoch 126, loss: 2.226109\n",
      "Epoch 127, loss: 2.236770\n",
      "Epoch 128, loss: 2.225564\n",
      "Epoch 129, loss: 2.237426\n",
      "Epoch 130, loss: 2.224301\n",
      "Epoch 131, loss: 2.201463\n",
      "Epoch 132, loss: 2.229087\n",
      "Epoch 133, loss: 2.230949\n",
      "Epoch 134, loss: 2.223326\n",
      "Epoch 135, loss: 2.238133\n",
      "Epoch 136, loss: 2.219299\n",
      "Epoch 137, loss: 2.214887\n",
      "Epoch 138, loss: 2.214674\n",
      "Epoch 139, loss: 2.233159\n",
      "Epoch 140, loss: 2.224177\n",
      "Epoch 141, loss: 2.203257\n",
      "Epoch 142, loss: 2.192372\n",
      "Epoch 143, loss: 2.206158\n",
      "Epoch 144, loss: 2.225630\n",
      "Epoch 145, loss: 2.231509\n",
      "Epoch 146, loss: 2.230521\n",
      "Epoch 147, loss: 2.204385\n",
      "Epoch 148, loss: 2.221147\n",
      "Epoch 149, loss: 2.208573\n",
      "Epoch 150, loss: 2.206936\n",
      "Epoch 151, loss: 2.210486\n",
      "Epoch 152, loss: 2.201696\n",
      "Epoch 153, loss: 2.202656\n",
      "Epoch 154, loss: 2.208470\n",
      "Epoch 155, loss: 2.221794\n",
      "Epoch 156, loss: 2.224973\n",
      "Epoch 157, loss: 2.209151\n",
      "Epoch 158, loss: 2.214747\n",
      "Epoch 159, loss: 2.229073\n",
      "Epoch 160, loss: 2.211383\n",
      "Epoch 161, loss: 2.228300\n",
      "Epoch 162, loss: 2.208344\n",
      "Epoch 163, loss: 2.222469\n",
      "Epoch 164, loss: 2.218295\n",
      "Epoch 165, loss: 2.183498\n",
      "Epoch 166, loss: 2.249894\n",
      "Epoch 167, loss: 2.237567\n",
      "Epoch 168, loss: 2.212682\n",
      "Epoch 169, loss: 2.234285\n",
      "Epoch 170, loss: 2.213474\n",
      "Epoch 171, loss: 2.210475\n",
      "Epoch 172, loss: 2.192961\n",
      "Epoch 173, loss: 2.220022\n",
      "Epoch 174, loss: 2.224239\n",
      "Epoch 175, loss: 2.194302\n",
      "Epoch 176, loss: 2.218800\n",
      "Epoch 177, loss: 2.203774\n",
      "Epoch 178, loss: 2.191061\n",
      "Epoch 179, loss: 2.186804\n",
      "Epoch 180, loss: 2.177347\n",
      "Epoch 181, loss: 2.197107\n",
      "Epoch 182, loss: 2.182350\n",
      "Epoch 183, loss: 2.230312\n",
      "Epoch 184, loss: 2.238152\n",
      "Epoch 185, loss: 2.210612\n",
      "Epoch 186, loss: 2.205986\n",
      "Epoch 187, loss: 2.234831\n",
      "Epoch 188, loss: 2.215201\n",
      "Epoch 189, loss: 2.212828\n",
      "Epoch 190, loss: 2.191057\n",
      "Epoch 191, loss: 2.219022\n",
      "Epoch 192, loss: 2.197949\n",
      "Epoch 193, loss: 2.193539\n",
      "Epoch 194, loss: 2.214316\n",
      "Epoch 195, loss: 2.212543\n",
      "Epoch 196, loss: 2.205257\n",
      "Epoch 197, loss: 2.190710\n",
      "Epoch 198, loss: 2.214378\n",
      "Epoch 199, loss: 2.201670\n",
      "Epoch 0, loss: 2.301773\n",
      "Epoch 1, loss: 2.301804\n",
      "Epoch 2, loss: 2.302839\n",
      "Epoch 3, loss: 2.302239\n",
      "Epoch 4, loss: 2.302117\n",
      "Epoch 5, loss: 2.301732\n",
      "Epoch 6, loss: 2.301992\n",
      "Epoch 7, loss: 2.301766\n",
      "Epoch 8, loss: 2.302157\n",
      "Epoch 9, loss: 2.302168\n",
      "Epoch 10, loss: 2.301887\n",
      "Epoch 11, loss: 2.301090\n",
      "Epoch 12, loss: 2.301616\n",
      "Epoch 13, loss: 2.301590\n",
      "Epoch 14, loss: 2.300642\n",
      "Epoch 15, loss: 2.301051\n",
      "Epoch 16, loss: 2.301052\n",
      "Epoch 17, loss: 2.300082\n",
      "Epoch 18, loss: 2.299901\n",
      "Epoch 19, loss: 2.298838\n",
      "Epoch 20, loss: 2.300352\n",
      "Epoch 21, loss: 2.300786\n",
      "Epoch 22, loss: 2.300157\n",
      "Epoch 23, loss: 2.300725\n",
      "Epoch 24, loss: 2.300300\n",
      "Epoch 25, loss: 2.299760\n",
      "Epoch 26, loss: 2.300233\n",
      "Epoch 27, loss: 2.299685\n",
      "Epoch 28, loss: 2.300434\n",
      "Epoch 29, loss: 2.299870\n",
      "Epoch 30, loss: 2.298180\n",
      "Epoch 31, loss: 2.298988\n",
      "Epoch 32, loss: 2.299467\n",
      "Epoch 33, loss: 2.298932\n",
      "Epoch 34, loss: 2.299553\n",
      "Epoch 35, loss: 2.300030\n",
      "Epoch 36, loss: 2.299651\n",
      "Epoch 37, loss: 2.298239\n",
      "Epoch 38, loss: 2.300594\n",
      "Epoch 39, loss: 2.299410\n",
      "Epoch 40, loss: 2.296188\n",
      "Epoch 41, loss: 2.298031\n",
      "Epoch 42, loss: 2.298343\n",
      "Epoch 43, loss: 2.296800\n",
      "Epoch 44, loss: 2.299685\n",
      "Epoch 45, loss: 2.300391\n",
      "Epoch 46, loss: 2.298081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, loss: 2.296639\n",
      "Epoch 48, loss: 2.298102\n",
      "Epoch 49, loss: 2.298137\n",
      "Epoch 50, loss: 2.298223\n",
      "Epoch 51, loss: 2.299227\n",
      "Epoch 52, loss: 2.299119\n",
      "Epoch 53, loss: 2.294599\n",
      "Epoch 54, loss: 2.296574\n",
      "Epoch 55, loss: 2.296178\n",
      "Epoch 56, loss: 2.299361\n",
      "Epoch 57, loss: 2.295370\n",
      "Epoch 58, loss: 2.295578\n",
      "Epoch 59, loss: 2.297272\n",
      "Epoch 60, loss: 2.297307\n",
      "Epoch 61, loss: 2.294495\n",
      "Epoch 62, loss: 2.296732\n",
      "Epoch 63, loss: 2.297255\n",
      "Epoch 64, loss: 2.297220\n",
      "Epoch 65, loss: 2.290756\n",
      "Epoch 66, loss: 2.297433\n",
      "Epoch 67, loss: 2.294615\n",
      "Epoch 68, loss: 2.296478\n",
      "Epoch 69, loss: 2.295760\n",
      "Epoch 70, loss: 2.299550\n",
      "Epoch 71, loss: 2.294862\n",
      "Epoch 72, loss: 2.295722\n",
      "Epoch 73, loss: 2.294649\n",
      "Epoch 74, loss: 2.294917\n",
      "Epoch 75, loss: 2.299169\n",
      "Epoch 76, loss: 2.297802\n",
      "Epoch 77, loss: 2.295887\n",
      "Epoch 78, loss: 2.294493\n",
      "Epoch 79, loss: 2.292776\n",
      "Epoch 80, loss: 2.294377\n",
      "Epoch 81, loss: 2.293690\n",
      "Epoch 82, loss: 2.293662\n",
      "Epoch 83, loss: 2.293918\n",
      "Epoch 84, loss: 2.296142\n",
      "Epoch 85, loss: 2.296990\n",
      "Epoch 86, loss: 2.297950\n",
      "Epoch 87, loss: 2.292517\n",
      "Epoch 88, loss: 2.294486\n",
      "Epoch 89, loss: 2.293744\n",
      "Epoch 90, loss: 2.295571\n",
      "Epoch 91, loss: 2.292814\n",
      "Epoch 92, loss: 2.293773\n",
      "Epoch 93, loss: 2.293438\n",
      "Epoch 94, loss: 2.294772\n",
      "Epoch 95, loss: 2.297696\n",
      "Epoch 96, loss: 2.288181\n",
      "Epoch 97, loss: 2.289434\n",
      "Epoch 98, loss: 2.293148\n",
      "Epoch 99, loss: 2.293770\n",
      "Epoch 100, loss: 2.295897\n",
      "Epoch 101, loss: 2.290137\n",
      "Epoch 102, loss: 2.293176\n",
      "Epoch 103, loss: 2.294740\n",
      "Epoch 104, loss: 2.293106\n",
      "Epoch 105, loss: 2.298013\n",
      "Epoch 106, loss: 2.289762\n",
      "Epoch 107, loss: 2.292681\n",
      "Epoch 108, loss: 2.292531\n",
      "Epoch 109, loss: 2.293154\n",
      "Epoch 110, loss: 2.292306\n",
      "Epoch 111, loss: 2.292716\n",
      "Epoch 112, loss: 2.288109\n",
      "Epoch 113, loss: 2.295040\n",
      "Epoch 114, loss: 2.290173\n",
      "Epoch 115, loss: 2.292155\n",
      "Epoch 116, loss: 2.289720\n",
      "Epoch 117, loss: 2.290349\n",
      "Epoch 118, loss: 2.296329\n",
      "Epoch 119, loss: 2.291151\n",
      "Epoch 120, loss: 2.290251\n",
      "Epoch 121, loss: 2.290158\n",
      "Epoch 122, loss: 2.290095\n",
      "Epoch 123, loss: 2.294397\n",
      "Epoch 124, loss: 2.293106\n",
      "Epoch 125, loss: 2.288776\n",
      "Epoch 126, loss: 2.288391\n",
      "Epoch 127, loss: 2.291909\n",
      "Epoch 128, loss: 2.286016\n",
      "Epoch 129, loss: 2.292270\n",
      "Epoch 130, loss: 2.291586\n",
      "Epoch 131, loss: 2.289007\n",
      "Epoch 132, loss: 2.291315\n",
      "Epoch 133, loss: 2.293877\n",
      "Epoch 134, loss: 2.288038\n",
      "Epoch 135, loss: 2.287985\n",
      "Epoch 136, loss: 2.290464\n",
      "Epoch 137, loss: 2.290033\n",
      "Epoch 138, loss: 2.288707\n",
      "Epoch 139, loss: 2.285693\n",
      "Epoch 140, loss: 2.292275\n",
      "Epoch 141, loss: 2.289247\n",
      "Epoch 142, loss: 2.290330\n",
      "Epoch 143, loss: 2.291911\n",
      "Epoch 144, loss: 2.294284\n",
      "Epoch 145, loss: 2.290241\n",
      "Epoch 146, loss: 2.292157\n",
      "Epoch 147, loss: 2.288614\n",
      "Epoch 148, loss: 2.290897\n",
      "Epoch 149, loss: 2.291327\n",
      "Epoch 150, loss: 2.292889\n",
      "Epoch 151, loss: 2.288809\n",
      "Epoch 152, loss: 2.292630\n",
      "Epoch 153, loss: 2.285793\n",
      "Epoch 154, loss: 2.288540\n",
      "Epoch 155, loss: 2.284130\n",
      "Epoch 156, loss: 2.289914\n",
      "Epoch 157, loss: 2.288670\n",
      "Epoch 158, loss: 2.283437\n",
      "Epoch 159, loss: 2.288284\n",
      "Epoch 160, loss: 2.284462\n",
      "Epoch 161, loss: 2.288652\n",
      "Epoch 162, loss: 2.287608\n",
      "Epoch 163, loss: 2.289658\n",
      "Epoch 164, loss: 2.283757\n",
      "Epoch 165, loss: 2.287055\n",
      "Epoch 166, loss: 2.286719\n",
      "Epoch 167, loss: 2.291573\n",
      "Epoch 168, loss: 2.289343\n",
      "Epoch 169, loss: 2.287327\n",
      "Epoch 170, loss: 2.285979\n",
      "Epoch 171, loss: 2.282985\n",
      "Epoch 172, loss: 2.291656\n",
      "Epoch 173, loss: 2.279164\n",
      "Epoch 174, loss: 2.287354\n",
      "Epoch 175, loss: 2.286841\n",
      "Epoch 176, loss: 2.289798\n",
      "Epoch 177, loss: 2.284027\n",
      "Epoch 178, loss: 2.287749\n",
      "Epoch 179, loss: 2.282116\n",
      "Epoch 180, loss: 2.287701\n",
      "Epoch 181, loss: 2.285032\n",
      "Epoch 182, loss: 2.284650\n",
      "Epoch 183, loss: 2.287863\n",
      "Epoch 184, loss: 2.284057\n",
      "Epoch 185, loss: 2.286868\n",
      "Epoch 186, loss: 2.286506\n",
      "Epoch 187, loss: 2.285814\n",
      "Epoch 188, loss: 2.291113\n",
      "Epoch 189, loss: 2.281095\n",
      "Epoch 190, loss: 2.281916\n",
      "Epoch 191, loss: 2.285374\n",
      "Epoch 192, loss: 2.282309\n",
      "Epoch 193, loss: 2.284128\n",
      "Epoch 194, loss: 2.281660\n",
      "Epoch 195, loss: 2.286631\n",
      "Epoch 196, loss: 2.291405\n",
      "Epoch 197, loss: 2.285768\n",
      "Epoch 198, loss: 2.289454\n",
      "Epoch 199, loss: 2.289846\n",
      "Epoch 0, loss: 2.302848\n",
      "Epoch 1, loss: 2.302364\n",
      "Epoch 2, loss: 2.302332\n",
      "Epoch 3, loss: 2.301635\n",
      "Epoch 4, loss: 2.301261\n",
      "Epoch 5, loss: 2.302085\n",
      "Epoch 6, loss: 2.302479\n",
      "Epoch 7, loss: 2.300777\n",
      "Epoch 8, loss: 2.300690\n",
      "Epoch 9, loss: 2.301700\n",
      "Epoch 10, loss: 2.302105\n",
      "Epoch 11, loss: 2.301545\n",
      "Epoch 12, loss: 2.301523\n",
      "Epoch 13, loss: 2.301330\n",
      "Epoch 14, loss: 2.302018\n",
      "Epoch 15, loss: 2.300394\n",
      "Epoch 16, loss: 2.300760\n",
      "Epoch 17, loss: 2.300537\n",
      "Epoch 18, loss: 2.301424\n",
      "Epoch 19, loss: 2.299925\n",
      "Epoch 20, loss: 2.299702\n",
      "Epoch 21, loss: 2.299106\n",
      "Epoch 22, loss: 2.301018\n",
      "Epoch 23, loss: 2.300741\n",
      "Epoch 24, loss: 2.299256\n",
      "Epoch 25, loss: 2.299952\n",
      "Epoch 26, loss: 2.299323\n",
      "Epoch 27, loss: 2.299425\n",
      "Epoch 28, loss: 2.300726\n",
      "Epoch 29, loss: 2.298865\n",
      "Epoch 30, loss: 2.300105\n",
      "Epoch 31, loss: 2.299787\n",
      "Epoch 32, loss: 2.299663\n",
      "Epoch 33, loss: 2.298825\n",
      "Epoch 34, loss: 2.299860\n",
      "Epoch 35, loss: 2.298456\n",
      "Epoch 36, loss: 2.301551\n",
      "Epoch 37, loss: 2.299271\n",
      "Epoch 38, loss: 2.298938\n",
      "Epoch 39, loss: 2.298990\n",
      "Epoch 40, loss: 2.298464\n",
      "Epoch 41, loss: 2.298814\n",
      "Epoch 42, loss: 2.298182\n",
      "Epoch 43, loss: 2.297779\n",
      "Epoch 44, loss: 2.298529\n",
      "Epoch 45, loss: 2.296739\n",
      "Epoch 46, loss: 2.300233\n",
      "Epoch 47, loss: 2.297989\n",
      "Epoch 48, loss: 2.299971\n",
      "Epoch 49, loss: 2.297450\n",
      "Epoch 50, loss: 2.297879\n",
      "Epoch 51, loss: 2.297603\n",
      "Epoch 52, loss: 2.298263\n",
      "Epoch 53, loss: 2.295781\n",
      "Epoch 54, loss: 2.296805\n",
      "Epoch 55, loss: 2.298900\n",
      "Epoch 56, loss: 2.294762\n",
      "Epoch 57, loss: 2.296570\n",
      "Epoch 58, loss: 2.298113\n",
      "Epoch 59, loss: 2.297005\n",
      "Epoch 60, loss: 2.295216\n",
      "Epoch 61, loss: 2.294887\n",
      "Epoch 62, loss: 2.295532\n",
      "Epoch 63, loss: 2.297542\n",
      "Epoch 64, loss: 2.293584\n",
      "Epoch 65, loss: 2.298998\n",
      "Epoch 66, loss: 2.295859\n",
      "Epoch 67, loss: 2.298289\n",
      "Epoch 68, loss: 2.292933\n",
      "Epoch 69, loss: 2.296863\n",
      "Epoch 70, loss: 2.294575\n",
      "Epoch 71, loss: 2.296397\n",
      "Epoch 72, loss: 2.294893\n",
      "Epoch 73, loss: 2.298685\n",
      "Epoch 74, loss: 2.294567\n",
      "Epoch 75, loss: 2.296488\n",
      "Epoch 76, loss: 2.295850\n",
      "Epoch 77, loss: 2.294595\n",
      "Epoch 78, loss: 2.294125\n",
      "Epoch 79, loss: 2.292552\n",
      "Epoch 80, loss: 2.293675\n",
      "Epoch 81, loss: 2.296621\n",
      "Epoch 82, loss: 2.295916\n",
      "Epoch 83, loss: 2.295306\n",
      "Epoch 84, loss: 2.293586\n",
      "Epoch 85, loss: 2.295620\n",
      "Epoch 86, loss: 2.291841\n",
      "Epoch 87, loss: 2.292239\n",
      "Epoch 88, loss: 2.290685\n",
      "Epoch 89, loss: 2.293104\n",
      "Epoch 90, loss: 2.296765\n",
      "Epoch 91, loss: 2.294367\n",
      "Epoch 92, loss: 2.296287\n",
      "Epoch 93, loss: 2.294888\n",
      "Epoch 94, loss: 2.291503\n",
      "Epoch 95, loss: 2.294612\n",
      "Epoch 96, loss: 2.294703\n",
      "Epoch 97, loss: 2.298774\n",
      "Epoch 98, loss: 2.296022\n",
      "Epoch 99, loss: 2.291963\n",
      "Epoch 100, loss: 2.293770\n",
      "Epoch 101, loss: 2.294066\n",
      "Epoch 102, loss: 2.291205\n",
      "Epoch 103, loss: 2.292832\n",
      "Epoch 104, loss: 2.295498\n",
      "Epoch 105, loss: 2.290582\n",
      "Epoch 106, loss: 2.295829\n",
      "Epoch 107, loss: 2.296568\n",
      "Epoch 108, loss: 2.292349\n",
      "Epoch 109, loss: 2.290034\n",
      "Epoch 110, loss: 2.288103\n",
      "Epoch 111, loss: 2.291918\n",
      "Epoch 112, loss: 2.295293\n",
      "Epoch 113, loss: 2.293358\n",
      "Epoch 114, loss: 2.293525\n",
      "Epoch 115, loss: 2.291149\n",
      "Epoch 116, loss: 2.291769\n",
      "Epoch 117, loss: 2.293634\n",
      "Epoch 118, loss: 2.294477\n",
      "Epoch 119, loss: 2.291958\n",
      "Epoch 120, loss: 2.293343\n",
      "Epoch 121, loss: 2.292307\n",
      "Epoch 122, loss: 2.288107\n",
      "Epoch 123, loss: 2.291391\n",
      "Epoch 124, loss: 2.290675\n",
      "Epoch 125, loss: 2.297626\n",
      "Epoch 126, loss: 2.284975\n",
      "Epoch 127, loss: 2.292975\n",
      "Epoch 128, loss: 2.295628\n",
      "Epoch 129, loss: 2.288985\n",
      "Epoch 130, loss: 2.286359\n",
      "Epoch 131, loss: 2.290820\n",
      "Epoch 132, loss: 2.290022\n",
      "Epoch 133, loss: 2.289444\n",
      "Epoch 134, loss: 2.294438\n",
      "Epoch 135, loss: 2.291419\n",
      "Epoch 136, loss: 2.293466\n",
      "Epoch 137, loss: 2.289945\n",
      "Epoch 138, loss: 2.291152\n",
      "Epoch 139, loss: 2.292802\n",
      "Epoch 140, loss: 2.290344\n",
      "Epoch 141, loss: 2.290861\n",
      "Epoch 142, loss: 2.290068\n",
      "Epoch 143, loss: 2.293475\n",
      "Epoch 144, loss: 2.289236\n",
      "Epoch 145, loss: 2.291304\n",
      "Epoch 146, loss: 2.290402\n",
      "Epoch 147, loss: 2.286446\n",
      "Epoch 148, loss: 2.288849\n",
      "Epoch 149, loss: 2.288777\n",
      "Epoch 150, loss: 2.291477\n",
      "Epoch 151, loss: 2.285855\n",
      "Epoch 152, loss: 2.287390\n",
      "Epoch 153, loss: 2.291269\n",
      "Epoch 154, loss: 2.284541\n",
      "Epoch 155, loss: 2.287171\n",
      "Epoch 156, loss: 2.289079\n",
      "Epoch 157, loss: 2.287364\n",
      "Epoch 158, loss: 2.288763\n",
      "Epoch 159, loss: 2.287981\n",
      "Epoch 160, loss: 2.288023\n",
      "Epoch 161, loss: 2.292352\n",
      "Epoch 162, loss: 2.288242\n",
      "Epoch 163, loss: 2.286306\n",
      "Epoch 164, loss: 2.291035\n",
      "Epoch 165, loss: 2.284612\n",
      "Epoch 166, loss: 2.291190\n",
      "Epoch 167, loss: 2.284186\n",
      "Epoch 168, loss: 2.285490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169, loss: 2.285285\n",
      "Epoch 170, loss: 2.285561\n",
      "Epoch 171, loss: 2.286340\n",
      "Epoch 172, loss: 2.289177\n",
      "Epoch 173, loss: 2.288402\n",
      "Epoch 174, loss: 2.287816\n",
      "Epoch 175, loss: 2.287579\n",
      "Epoch 176, loss: 2.284685\n",
      "Epoch 177, loss: 2.282519\n",
      "Epoch 178, loss: 2.290575\n",
      "Epoch 179, loss: 2.292186\n",
      "Epoch 180, loss: 2.283190\n",
      "Epoch 181, loss: 2.288784\n",
      "Epoch 182, loss: 2.285092\n",
      "Epoch 183, loss: 2.286018\n",
      "Epoch 184, loss: 2.287551\n",
      "Epoch 185, loss: 2.289593\n",
      "Epoch 186, loss: 2.282530\n",
      "Epoch 187, loss: 2.284872\n",
      "Epoch 188, loss: 2.287857\n",
      "Epoch 189, loss: 2.283141\n",
      "Epoch 190, loss: 2.287442\n",
      "Epoch 191, loss: 2.282361\n",
      "Epoch 192, loss: 2.289573\n",
      "Epoch 193, loss: 2.284836\n",
      "Epoch 194, loss: 2.285441\n",
      "Epoch 195, loss: 2.285944\n",
      "Epoch 196, loss: 2.283469\n",
      "Epoch 197, loss: 2.287642\n",
      "Epoch 198, loss: 2.286929\n",
      "Epoch 199, loss: 2.286377\n",
      "Epoch 0, loss: 2.301853\n",
      "Epoch 1, loss: 2.302601\n",
      "Epoch 2, loss: 2.302456\n",
      "Epoch 3, loss: 2.302250\n",
      "Epoch 4, loss: 2.302425\n",
      "Epoch 5, loss: 2.303251\n",
      "Epoch 6, loss: 2.301117\n",
      "Epoch 7, loss: 2.302780\n",
      "Epoch 8, loss: 2.301938\n",
      "Epoch 9, loss: 2.301157\n",
      "Epoch 10, loss: 2.300090\n",
      "Epoch 11, loss: 2.301521\n",
      "Epoch 12, loss: 2.301713\n",
      "Epoch 13, loss: 2.301013\n",
      "Epoch 14, loss: 2.301923\n",
      "Epoch 15, loss: 2.299967\n",
      "Epoch 16, loss: 2.301778\n",
      "Epoch 17, loss: 2.300745\n",
      "Epoch 18, loss: 2.300139\n",
      "Epoch 19, loss: 2.300487\n",
      "Epoch 20, loss: 2.300480\n",
      "Epoch 21, loss: 2.301334\n",
      "Epoch 22, loss: 2.299845\n",
      "Epoch 23, loss: 2.300958\n",
      "Epoch 24, loss: 2.299365\n",
      "Epoch 25, loss: 2.301857\n",
      "Epoch 26, loss: 2.300065\n",
      "Epoch 27, loss: 2.299031\n",
      "Epoch 28, loss: 2.299864\n",
      "Epoch 29, loss: 2.297469\n",
      "Epoch 30, loss: 2.299444\n",
      "Epoch 31, loss: 2.300192\n",
      "Epoch 32, loss: 2.302455\n",
      "Epoch 33, loss: 2.300406\n",
      "Epoch 34, loss: 2.299487\n",
      "Epoch 35, loss: 2.299082\n",
      "Epoch 36, loss: 2.297943\n",
      "Epoch 37, loss: 2.300895\n",
      "Epoch 38, loss: 2.297490\n",
      "Epoch 39, loss: 2.296351\n",
      "Epoch 40, loss: 2.299728\n",
      "Epoch 41, loss: 2.296747\n",
      "Epoch 42, loss: 2.298539\n",
      "Epoch 43, loss: 2.298928\n",
      "Epoch 44, loss: 2.298072\n",
      "Epoch 45, loss: 2.296507\n",
      "Epoch 46, loss: 2.297581\n",
      "Epoch 47, loss: 2.299918\n",
      "Epoch 48, loss: 2.297888\n",
      "Epoch 49, loss: 2.298961\n",
      "Epoch 50, loss: 2.298879\n",
      "Epoch 51, loss: 2.297389\n",
      "Epoch 52, loss: 2.297061\n",
      "Epoch 53, loss: 2.300726\n",
      "Epoch 54, loss: 2.295951\n",
      "Epoch 55, loss: 2.296198\n",
      "Epoch 56, loss: 2.298861\n",
      "Epoch 57, loss: 2.296971\n",
      "Epoch 58, loss: 2.298592\n",
      "Epoch 59, loss: 2.298299\n",
      "Epoch 60, loss: 2.296695\n",
      "Epoch 61, loss: 2.296344\n",
      "Epoch 62, loss: 2.299215\n",
      "Epoch 63, loss: 2.298175\n",
      "Epoch 64, loss: 2.296525\n",
      "Epoch 65, loss: 2.293474\n",
      "Epoch 66, loss: 2.296440\n",
      "Epoch 67, loss: 2.300189\n",
      "Epoch 68, loss: 2.295738\n",
      "Epoch 69, loss: 2.297256\n",
      "Epoch 70, loss: 2.296957\n",
      "Epoch 71, loss: 2.295665\n",
      "Epoch 72, loss: 2.294970\n",
      "Epoch 73, loss: 2.295383\n",
      "Epoch 74, loss: 2.292288\n",
      "Epoch 75, loss: 2.295515\n",
      "Epoch 76, loss: 2.294616\n",
      "Epoch 77, loss: 2.297982\n",
      "Epoch 78, loss: 2.293976\n",
      "Epoch 79, loss: 2.292553\n",
      "Epoch 80, loss: 2.297543\n",
      "Epoch 81, loss: 2.294114\n",
      "Epoch 82, loss: 2.296402\n",
      "Epoch 83, loss: 2.295042\n",
      "Epoch 84, loss: 2.291903\n",
      "Epoch 85, loss: 2.292106\n",
      "Epoch 86, loss: 2.294717\n",
      "Epoch 87, loss: 2.295837\n",
      "Epoch 88, loss: 2.295010\n",
      "Epoch 89, loss: 2.296975\n",
      "Epoch 90, loss: 2.290559\n",
      "Epoch 91, loss: 2.290664\n",
      "Epoch 92, loss: 2.293574\n",
      "Epoch 93, loss: 2.294981\n",
      "Epoch 94, loss: 2.296218\n",
      "Epoch 95, loss: 2.292650\n",
      "Epoch 96, loss: 2.295765\n",
      "Epoch 97, loss: 2.295707\n",
      "Epoch 98, loss: 2.292475\n",
      "Epoch 99, loss: 2.294026\n",
      "Epoch 100, loss: 2.291909\n",
      "Epoch 101, loss: 2.293613\n",
      "Epoch 102, loss: 2.291795\n",
      "Epoch 103, loss: 2.293976\n",
      "Epoch 104, loss: 2.293012\n",
      "Epoch 105, loss: 2.294555\n",
      "Epoch 106, loss: 2.292205\n",
      "Epoch 107, loss: 2.293484\n",
      "Epoch 108, loss: 2.294628\n",
      "Epoch 109, loss: 2.296699\n",
      "Epoch 110, loss: 2.289482\n",
      "Epoch 111, loss: 2.289744\n",
      "Epoch 112, loss: 2.292407\n",
      "Epoch 113, loss: 2.293313\n",
      "Epoch 114, loss: 2.289615\n",
      "Epoch 115, loss: 2.292816\n",
      "Epoch 116, loss: 2.293870\n",
      "Epoch 117, loss: 2.289524\n",
      "Epoch 118, loss: 2.290316\n",
      "Epoch 119, loss: 2.295660\n",
      "Epoch 120, loss: 2.293226\n",
      "Epoch 121, loss: 2.296676\n",
      "Epoch 122, loss: 2.293360\n",
      "Epoch 123, loss: 2.290319\n",
      "Epoch 124, loss: 2.288549\n",
      "Epoch 125, loss: 2.292743\n",
      "Epoch 126, loss: 2.290758\n",
      "Epoch 127, loss: 2.291870\n",
      "Epoch 128, loss: 2.287180\n",
      "Epoch 129, loss: 2.293310\n",
      "Epoch 130, loss: 2.297197\n",
      "Epoch 131, loss: 2.291501\n",
      "Epoch 132, loss: 2.289171\n",
      "Epoch 133, loss: 2.289378\n",
      "Epoch 134, loss: 2.288493\n",
      "Epoch 135, loss: 2.289880\n",
      "Epoch 136, loss: 2.291675\n",
      "Epoch 137, loss: 2.289641\n",
      "Epoch 138, loss: 2.287909\n",
      "Epoch 139, loss: 2.290260\n",
      "Epoch 140, loss: 2.291672\n",
      "Epoch 141, loss: 2.288387\n",
      "Epoch 142, loss: 2.294091\n",
      "Epoch 143, loss: 2.289556\n",
      "Epoch 144, loss: 2.289793\n",
      "Epoch 145, loss: 2.291462\n",
      "Epoch 146, loss: 2.286892\n",
      "Epoch 147, loss: 2.288899\n",
      "Epoch 148, loss: 2.294096\n",
      "Epoch 149, loss: 2.289833\n",
      "Epoch 150, loss: 2.285185\n",
      "Epoch 151, loss: 2.287663\n",
      "Epoch 152, loss: 2.286870\n",
      "Epoch 153, loss: 2.289036\n",
      "Epoch 154, loss: 2.288225\n",
      "Epoch 155, loss: 2.289838\n",
      "Epoch 156, loss: 2.289710\n",
      "Epoch 157, loss: 2.290011\n",
      "Epoch 158, loss: 2.290236\n",
      "Epoch 159, loss: 2.288697\n",
      "Epoch 160, loss: 2.285827\n",
      "Epoch 161, loss: 2.289499\n",
      "Epoch 162, loss: 2.285457\n",
      "Epoch 163, loss: 2.285105\n",
      "Epoch 164, loss: 2.287867\n",
      "Epoch 165, loss: 2.286854\n",
      "Epoch 166, loss: 2.291687\n",
      "Epoch 167, loss: 2.285812\n",
      "Epoch 168, loss: 2.287732\n",
      "Epoch 169, loss: 2.290004\n",
      "Epoch 170, loss: 2.285720\n",
      "Epoch 171, loss: 2.289849\n",
      "Epoch 172, loss: 2.284734\n",
      "Epoch 173, loss: 2.291609\n",
      "Epoch 174, loss: 2.286428\n",
      "Epoch 175, loss: 2.285961\n",
      "Epoch 176, loss: 2.287559\n",
      "Epoch 177, loss: 2.284718\n",
      "Epoch 178, loss: 2.281192\n",
      "Epoch 179, loss: 2.290161\n",
      "Epoch 180, loss: 2.289061\n",
      "Epoch 181, loss: 2.291652\n",
      "Epoch 182, loss: 2.290753\n",
      "Epoch 183, loss: 2.291516\n",
      "Epoch 184, loss: 2.286810\n",
      "Epoch 185, loss: 2.283569\n",
      "Epoch 186, loss: 2.286747\n",
      "Epoch 187, loss: 2.289758\n",
      "Epoch 188, loss: 2.286466\n",
      "Epoch 189, loss: 2.280817\n",
      "Epoch 190, loss: 2.284729\n",
      "Epoch 191, loss: 2.282119\n",
      "Epoch 192, loss: 2.282291\n",
      "Epoch 193, loss: 2.295581\n",
      "Epoch 194, loss: 2.286579\n",
      "Epoch 195, loss: 2.284784\n",
      "Epoch 196, loss: 2.283905\n",
      "Epoch 197, loss: 2.287154\n",
      "Epoch 198, loss: 2.288119\n",
      "Epoch 199, loss: 2.287856\n",
      "Epoch 0, loss: 2.302775\n",
      "Epoch 1, loss: 2.303592\n",
      "Epoch 2, loss: 2.302861\n",
      "Epoch 3, loss: 2.301942\n",
      "Epoch 4, loss: 2.301982\n",
      "Epoch 5, loss: 2.302075\n",
      "Epoch 6, loss: 2.302611\n",
      "Epoch 7, loss: 2.302584\n",
      "Epoch 8, loss: 2.302899\n",
      "Epoch 9, loss: 2.302207\n",
      "Epoch 10, loss: 2.302500\n",
      "Epoch 11, loss: 2.302612\n",
      "Epoch 12, loss: 2.302576\n",
      "Epoch 13, loss: 2.302661\n",
      "Epoch 14, loss: 2.302789\n",
      "Epoch 15, loss: 2.302965\n",
      "Epoch 16, loss: 2.302436\n",
      "Epoch 17, loss: 2.302878\n",
      "Epoch 18, loss: 2.303082\n",
      "Epoch 19, loss: 2.302250\n",
      "Epoch 20, loss: 2.302805\n",
      "Epoch 21, loss: 2.302953\n",
      "Epoch 22, loss: 2.302003\n",
      "Epoch 23, loss: 2.302971\n",
      "Epoch 24, loss: 2.301210\n",
      "Epoch 25, loss: 2.302635\n",
      "Epoch 26, loss: 2.303843\n",
      "Epoch 27, loss: 2.302150\n",
      "Epoch 28, loss: 2.302502\n",
      "Epoch 29, loss: 2.302528\n",
      "Epoch 30, loss: 2.302401\n",
      "Epoch 31, loss: 2.301777\n",
      "Epoch 32, loss: 2.302390\n",
      "Epoch 33, loss: 2.302937\n",
      "Epoch 34, loss: 2.301597\n",
      "Epoch 35, loss: 2.301921\n",
      "Epoch 36, loss: 2.302425\n",
      "Epoch 37, loss: 2.302185\n",
      "Epoch 38, loss: 2.303682\n",
      "Epoch 39, loss: 2.302295\n",
      "Epoch 40, loss: 2.301508\n",
      "Epoch 41, loss: 2.302204\n",
      "Epoch 42, loss: 2.302305\n",
      "Epoch 43, loss: 2.302938\n",
      "Epoch 44, loss: 2.301980\n",
      "Epoch 45, loss: 2.302603\n",
      "Epoch 46, loss: 2.302496\n",
      "Epoch 47, loss: 2.301952\n",
      "Epoch 48, loss: 2.301102\n",
      "Epoch 49, loss: 2.301904\n",
      "Epoch 50, loss: 2.301298\n",
      "Epoch 51, loss: 2.302330\n",
      "Epoch 52, loss: 2.302363\n",
      "Epoch 53, loss: 2.301947\n",
      "Epoch 54, loss: 2.301651\n",
      "Epoch 55, loss: 2.300563\n",
      "Epoch 56, loss: 2.301818\n",
      "Epoch 57, loss: 2.301110\n",
      "Epoch 58, loss: 2.301677\n",
      "Epoch 59, loss: 2.301084\n",
      "Epoch 60, loss: 2.301976\n",
      "Epoch 61, loss: 2.301393\n",
      "Epoch 62, loss: 2.301988\n",
      "Epoch 63, loss: 2.301819\n",
      "Epoch 64, loss: 2.301857\n",
      "Epoch 65, loss: 2.301506\n",
      "Epoch 66, loss: 2.302719\n",
      "Epoch 67, loss: 2.302288\n",
      "Epoch 68, loss: 2.302480\n",
      "Epoch 69, loss: 2.301612\n",
      "Epoch 70, loss: 2.302364\n",
      "Epoch 71, loss: 2.300999\n",
      "Epoch 72, loss: 2.301764\n",
      "Epoch 73, loss: 2.301079\n",
      "Epoch 74, loss: 2.302505\n",
      "Epoch 75, loss: 2.301076\n",
      "Epoch 76, loss: 2.303049\n",
      "Epoch 77, loss: 2.301487\n",
      "Epoch 78, loss: 2.301563\n",
      "Epoch 79, loss: 2.302322\n",
      "Epoch 80, loss: 2.301277\n",
      "Epoch 81, loss: 2.303303\n",
      "Epoch 82, loss: 2.302515\n",
      "Epoch 83, loss: 2.301083\n",
      "Epoch 84, loss: 2.301480\n",
      "Epoch 85, loss: 2.302464\n",
      "Epoch 86, loss: 2.302498\n",
      "Epoch 87, loss: 2.301831\n",
      "Epoch 88, loss: 2.301767\n",
      "Epoch 89, loss: 2.301193\n",
      "Epoch 90, loss: 2.301565\n",
      "Epoch 91, loss: 2.301845\n",
      "Epoch 92, loss: 2.299488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, loss: 2.302128\n",
      "Epoch 94, loss: 2.301480\n",
      "Epoch 95, loss: 2.301590\n",
      "Epoch 96, loss: 2.302089\n",
      "Epoch 97, loss: 2.301246\n",
      "Epoch 98, loss: 2.301164\n",
      "Epoch 99, loss: 2.301148\n",
      "Epoch 100, loss: 2.300879\n",
      "Epoch 101, loss: 2.302188\n",
      "Epoch 102, loss: 2.302039\n",
      "Epoch 103, loss: 2.301434\n",
      "Epoch 104, loss: 2.300862\n",
      "Epoch 105, loss: 2.301377\n",
      "Epoch 106, loss: 2.300593\n",
      "Epoch 107, loss: 2.301437\n",
      "Epoch 108, loss: 2.301642\n",
      "Epoch 109, loss: 2.302317\n",
      "Epoch 110, loss: 2.301968\n",
      "Epoch 111, loss: 2.300754\n",
      "Epoch 112, loss: 2.301110\n",
      "Epoch 113, loss: 2.300311\n",
      "Epoch 114, loss: 2.301961\n",
      "Epoch 115, loss: 2.300576\n",
      "Epoch 116, loss: 2.301585\n",
      "Epoch 117, loss: 2.301451\n",
      "Epoch 118, loss: 2.301843\n",
      "Epoch 119, loss: 2.300279\n",
      "Epoch 120, loss: 2.301399\n",
      "Epoch 121, loss: 2.301162\n",
      "Epoch 122, loss: 2.300106\n",
      "Epoch 123, loss: 2.300667\n",
      "Epoch 124, loss: 2.301789\n",
      "Epoch 125, loss: 2.300445\n",
      "Epoch 126, loss: 2.302487\n",
      "Epoch 127, loss: 2.302105\n",
      "Epoch 128, loss: 2.301873\n",
      "Epoch 129, loss: 2.301786\n",
      "Epoch 130, loss: 2.300430\n",
      "Epoch 131, loss: 2.300958\n",
      "Epoch 132, loss: 2.301137\n",
      "Epoch 133, loss: 2.300784\n",
      "Epoch 134, loss: 2.301629\n",
      "Epoch 135, loss: 2.300574\n",
      "Epoch 136, loss: 2.301815\n",
      "Epoch 137, loss: 2.302304\n",
      "Epoch 138, loss: 2.301428\n",
      "Epoch 139, loss: 2.301059\n",
      "Epoch 140, loss: 2.302713\n",
      "Epoch 141, loss: 2.300652\n",
      "Epoch 142, loss: 2.302940\n",
      "Epoch 143, loss: 2.300788\n",
      "Epoch 144, loss: 2.300364\n",
      "Epoch 145, loss: 2.301803\n",
      "Epoch 146, loss: 2.301055\n",
      "Epoch 147, loss: 2.300453\n",
      "Epoch 148, loss: 2.300547\n",
      "Epoch 149, loss: 2.300560\n",
      "Epoch 150, loss: 2.300810\n",
      "Epoch 151, loss: 2.301576\n",
      "Epoch 152, loss: 2.300715\n",
      "Epoch 153, loss: 2.301310\n",
      "Epoch 154, loss: 2.300352\n",
      "Epoch 155, loss: 2.299681\n",
      "Epoch 156, loss: 2.301466\n",
      "Epoch 157, loss: 2.300610\n",
      "Epoch 158, loss: 2.300966\n",
      "Epoch 159, loss: 2.299702\n",
      "Epoch 160, loss: 2.299946\n",
      "Epoch 161, loss: 2.300625\n",
      "Epoch 162, loss: 2.300972\n",
      "Epoch 163, loss: 2.300805\n",
      "Epoch 164, loss: 2.299680\n",
      "Epoch 165, loss: 2.300950\n",
      "Epoch 166, loss: 2.301888\n",
      "Epoch 167, loss: 2.300845\n",
      "Epoch 168, loss: 2.300450\n",
      "Epoch 169, loss: 2.301287\n",
      "Epoch 170, loss: 2.301020\n",
      "Epoch 171, loss: 2.299516\n",
      "Epoch 172, loss: 2.300845\n",
      "Epoch 173, loss: 2.301206\n",
      "Epoch 174, loss: 2.301574\n",
      "Epoch 175, loss: 2.301502\n",
      "Epoch 176, loss: 2.300019\n",
      "Epoch 177, loss: 2.300056\n",
      "Epoch 178, loss: 2.300990\n",
      "Epoch 179, loss: 2.301234\n",
      "Epoch 180, loss: 2.300996\n",
      "Epoch 181, loss: 2.301100\n",
      "Epoch 182, loss: 2.300255\n",
      "Epoch 183, loss: 2.299911\n",
      "Epoch 184, loss: 2.301784\n",
      "Epoch 185, loss: 2.300745\n",
      "Epoch 186, loss: 2.300219\n",
      "Epoch 187, loss: 2.299427\n",
      "Epoch 188, loss: 2.300394\n",
      "Epoch 189, loss: 2.301346\n",
      "Epoch 190, loss: 2.299800\n",
      "Epoch 191, loss: 2.299882\n",
      "Epoch 192, loss: 2.300929\n",
      "Epoch 193, loss: 2.300970\n",
      "Epoch 194, loss: 2.300135\n",
      "Epoch 195, loss: 2.300319\n",
      "Epoch 196, loss: 2.301344\n",
      "Epoch 197, loss: 2.302408\n",
      "Epoch 198, loss: 2.300116\n",
      "Epoch 199, loss: 2.300890\n",
      "Epoch 0, loss: 2.302133\n",
      "Epoch 1, loss: 2.302946\n",
      "Epoch 2, loss: 2.302879\n",
      "Epoch 3, loss: 2.303658\n",
      "Epoch 4, loss: 2.303318\n",
      "Epoch 5, loss: 2.301630\n",
      "Epoch 6, loss: 2.301661\n",
      "Epoch 7, loss: 2.302038\n",
      "Epoch 8, loss: 2.302725\n",
      "Epoch 9, loss: 2.301360\n",
      "Epoch 10, loss: 2.302860\n",
      "Epoch 11, loss: 2.302945\n",
      "Epoch 12, loss: 2.302742\n",
      "Epoch 13, loss: 2.301668\n",
      "Epoch 14, loss: 2.301769\n",
      "Epoch 15, loss: 2.302659\n",
      "Epoch 16, loss: 2.302973\n",
      "Epoch 17, loss: 2.301400\n",
      "Epoch 18, loss: 2.302479\n",
      "Epoch 19, loss: 2.302660\n",
      "Epoch 20, loss: 2.302796\n",
      "Epoch 21, loss: 2.302164\n",
      "Epoch 22, loss: 2.302594\n",
      "Epoch 23, loss: 2.303413\n",
      "Epoch 24, loss: 2.302766\n",
      "Epoch 25, loss: 2.302437\n",
      "Epoch 26, loss: 2.302886\n",
      "Epoch 27, loss: 2.302055\n",
      "Epoch 28, loss: 2.302725\n",
      "Epoch 29, loss: 2.302417\n",
      "Epoch 30, loss: 2.301906\n",
      "Epoch 31, loss: 2.302062\n",
      "Epoch 32, loss: 2.302048\n",
      "Epoch 33, loss: 2.301432\n",
      "Epoch 34, loss: 2.300573\n",
      "Epoch 35, loss: 2.301218\n",
      "Epoch 36, loss: 2.302336\n",
      "Epoch 37, loss: 2.302673\n",
      "Epoch 38, loss: 2.301824\n",
      "Epoch 39, loss: 2.301962\n",
      "Epoch 40, loss: 2.301639\n",
      "Epoch 41, loss: 2.302696\n",
      "Epoch 42, loss: 2.302018\n",
      "Epoch 43, loss: 2.302044\n",
      "Epoch 44, loss: 2.301666\n",
      "Epoch 45, loss: 2.301678\n",
      "Epoch 46, loss: 2.301064\n",
      "Epoch 47, loss: 2.302460\n",
      "Epoch 48, loss: 2.301307\n",
      "Epoch 49, loss: 2.302604\n",
      "Epoch 50, loss: 2.302322\n",
      "Epoch 51, loss: 2.301969\n",
      "Epoch 52, loss: 2.302229\n",
      "Epoch 53, loss: 2.301945\n",
      "Epoch 54, loss: 2.301428\n",
      "Epoch 55, loss: 2.301778\n",
      "Epoch 56, loss: 2.302620\n",
      "Epoch 57, loss: 2.302596\n",
      "Epoch 58, loss: 2.301651\n",
      "Epoch 59, loss: 2.300870\n",
      "Epoch 60, loss: 2.302028\n",
      "Epoch 61, loss: 2.301341\n",
      "Epoch 62, loss: 2.302473\n",
      "Epoch 63, loss: 2.302525\n",
      "Epoch 64, loss: 2.301349\n",
      "Epoch 65, loss: 2.302038\n",
      "Epoch 66, loss: 2.301373\n",
      "Epoch 67, loss: 2.302054\n",
      "Epoch 68, loss: 2.302717\n",
      "Epoch 69, loss: 2.302700\n",
      "Epoch 70, loss: 2.301188\n",
      "Epoch 71, loss: 2.301854\n",
      "Epoch 72, loss: 2.300228\n",
      "Epoch 73, loss: 2.302827\n",
      "Epoch 74, loss: 2.302543\n",
      "Epoch 75, loss: 2.302170\n",
      "Epoch 76, loss: 2.301719\n",
      "Epoch 77, loss: 2.302178\n",
      "Epoch 78, loss: 2.301143\n",
      "Epoch 79, loss: 2.301662\n",
      "Epoch 80, loss: 2.301392\n",
      "Epoch 81, loss: 2.302269\n",
      "Epoch 82, loss: 2.301208\n",
      "Epoch 83, loss: 2.302121\n",
      "Epoch 84, loss: 2.301947\n",
      "Epoch 85, loss: 2.301060\n",
      "Epoch 86, loss: 2.302000\n",
      "Epoch 87, loss: 2.301577\n",
      "Epoch 88, loss: 2.302010\n",
      "Epoch 89, loss: 2.302051\n",
      "Epoch 90, loss: 2.303637\n",
      "Epoch 91, loss: 2.301180\n",
      "Epoch 92, loss: 2.302560\n",
      "Epoch 93, loss: 2.301275\n",
      "Epoch 94, loss: 2.302481\n",
      "Epoch 95, loss: 2.300723\n",
      "Epoch 96, loss: 2.302065\n",
      "Epoch 97, loss: 2.301039\n",
      "Epoch 98, loss: 2.302193\n",
      "Epoch 99, loss: 2.300702\n",
      "Epoch 100, loss: 2.300072\n",
      "Epoch 101, loss: 2.301111\n",
      "Epoch 102, loss: 2.300737\n",
      "Epoch 103, loss: 2.302365\n",
      "Epoch 104, loss: 2.302458\n",
      "Epoch 105, loss: 2.301854\n",
      "Epoch 106, loss: 2.300969\n",
      "Epoch 107, loss: 2.301382\n",
      "Epoch 108, loss: 2.301477\n",
      "Epoch 109, loss: 2.301073\n",
      "Epoch 110, loss: 2.300156\n",
      "Epoch 111, loss: 2.302292\n",
      "Epoch 112, loss: 2.301564\n",
      "Epoch 113, loss: 2.300804\n",
      "Epoch 114, loss: 2.301155\n",
      "Epoch 115, loss: 2.300939\n",
      "Epoch 116, loss: 2.302361\n",
      "Epoch 117, loss: 2.301492\n",
      "Epoch 118, loss: 2.301172\n",
      "Epoch 119, loss: 2.301957\n",
      "Epoch 120, loss: 2.301784\n",
      "Epoch 121, loss: 2.302048\n",
      "Epoch 122, loss: 2.301398\n",
      "Epoch 123, loss: 2.301289\n",
      "Epoch 124, loss: 2.300231\n",
      "Epoch 125, loss: 2.301546\n",
      "Epoch 126, loss: 2.300554\n",
      "Epoch 127, loss: 2.300114\n",
      "Epoch 128, loss: 2.302071\n",
      "Epoch 129, loss: 2.300845\n",
      "Epoch 130, loss: 2.300342\n",
      "Epoch 131, loss: 2.300692\n",
      "Epoch 132, loss: 2.300953\n",
      "Epoch 133, loss: 2.301221\n",
      "Epoch 134, loss: 2.301030\n",
      "Epoch 135, loss: 2.301118\n",
      "Epoch 136, loss: 2.300377\n",
      "Epoch 137, loss: 2.302856\n",
      "Epoch 138, loss: 2.301435\n",
      "Epoch 139, loss: 2.300850\n",
      "Epoch 140, loss: 2.302031\n",
      "Epoch 141, loss: 2.301404\n",
      "Epoch 142, loss: 2.301772\n",
      "Epoch 143, loss: 2.300887\n",
      "Epoch 144, loss: 2.299714\n",
      "Epoch 145, loss: 2.300652\n",
      "Epoch 146, loss: 2.301717\n",
      "Epoch 147, loss: 2.302366\n",
      "Epoch 148, loss: 2.300469\n",
      "Epoch 149, loss: 2.301069\n",
      "Epoch 150, loss: 2.300997\n",
      "Epoch 151, loss: 2.299689\n",
      "Epoch 152, loss: 2.301765\n",
      "Epoch 153, loss: 2.300720\n",
      "Epoch 154, loss: 2.301394\n",
      "Epoch 155, loss: 2.300620\n",
      "Epoch 156, loss: 2.301489\n",
      "Epoch 157, loss: 2.300965\n",
      "Epoch 158, loss: 2.301849\n",
      "Epoch 159, loss: 2.300929\n",
      "Epoch 160, loss: 2.300965\n",
      "Epoch 161, loss: 2.302019\n",
      "Epoch 162, loss: 2.300757\n",
      "Epoch 163, loss: 2.299450\n",
      "Epoch 164, loss: 2.300042\n",
      "Epoch 165, loss: 2.300019\n",
      "Epoch 166, loss: 2.301071\n",
      "Epoch 167, loss: 2.300387\n",
      "Epoch 168, loss: 2.299439\n",
      "Epoch 169, loss: 2.300282\n",
      "Epoch 170, loss: 2.300897\n",
      "Epoch 171, loss: 2.301880\n",
      "Epoch 172, loss: 2.300200\n",
      "Epoch 173, loss: 2.300663\n",
      "Epoch 174, loss: 2.300318\n",
      "Epoch 175, loss: 2.302247\n",
      "Epoch 176, loss: 2.300263\n",
      "Epoch 177, loss: 2.300075\n",
      "Epoch 178, loss: 2.299129\n",
      "Epoch 179, loss: 2.301733\n",
      "Epoch 180, loss: 2.298706\n",
      "Epoch 181, loss: 2.300821\n",
      "Epoch 182, loss: 2.301459\n",
      "Epoch 183, loss: 2.300143\n",
      "Epoch 184, loss: 2.301893\n",
      "Epoch 185, loss: 2.300427\n",
      "Epoch 186, loss: 2.300392\n",
      "Epoch 187, loss: 2.300208\n",
      "Epoch 188, loss: 2.301508\n",
      "Epoch 189, loss: 2.301971\n",
      "Epoch 190, loss: 2.300396\n",
      "Epoch 191, loss: 2.301390\n",
      "Epoch 192, loss: 2.300605\n",
      "Epoch 193, loss: 2.300721\n",
      "Epoch 194, loss: 2.299770\n",
      "Epoch 195, loss: 2.299451\n",
      "Epoch 196, loss: 2.300246\n",
      "Epoch 197, loss: 2.301016\n",
      "Epoch 198, loss: 2.301588\n",
      "Epoch 199, loss: 2.299325\n",
      "Epoch 0, loss: 2.301513\n",
      "Epoch 1, loss: 2.301854\n",
      "Epoch 2, loss: 2.302568\n",
      "Epoch 3, loss: 2.301802\n",
      "Epoch 4, loss: 2.303665\n",
      "Epoch 5, loss: 2.303789\n",
      "Epoch 6, loss: 2.302695\n",
      "Epoch 7, loss: 2.303127\n",
      "Epoch 8, loss: 2.301125\n",
      "Epoch 9, loss: 2.301812\n",
      "Epoch 10, loss: 2.302544\n",
      "Epoch 11, loss: 2.302949\n",
      "Epoch 12, loss: 2.302440\n",
      "Epoch 13, loss: 2.302485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, loss: 2.302560\n",
      "Epoch 15, loss: 2.300673\n",
      "Epoch 16, loss: 2.301970\n",
      "Epoch 17, loss: 2.302285\n",
      "Epoch 18, loss: 2.301764\n",
      "Epoch 19, loss: 2.302280\n",
      "Epoch 20, loss: 2.302591\n",
      "Epoch 21, loss: 2.302097\n",
      "Epoch 22, loss: 2.302381\n",
      "Epoch 23, loss: 2.302059\n",
      "Epoch 24, loss: 2.302042\n",
      "Epoch 25, loss: 2.301430\n",
      "Epoch 26, loss: 2.303331\n",
      "Epoch 27, loss: 2.302868\n",
      "Epoch 28, loss: 2.302145\n",
      "Epoch 29, loss: 2.301158\n",
      "Epoch 30, loss: 2.302509\n",
      "Epoch 31, loss: 2.301251\n",
      "Epoch 32, loss: 2.302226\n",
      "Epoch 33, loss: 2.302343\n",
      "Epoch 34, loss: 2.301434\n",
      "Epoch 35, loss: 2.303526\n",
      "Epoch 36, loss: 2.302074\n",
      "Epoch 37, loss: 2.301246\n",
      "Epoch 38, loss: 2.301849\n",
      "Epoch 39, loss: 2.301813\n",
      "Epoch 40, loss: 2.301988\n",
      "Epoch 41, loss: 2.301907\n",
      "Epoch 42, loss: 2.302486\n",
      "Epoch 43, loss: 2.301245\n",
      "Epoch 44, loss: 2.301778\n",
      "Epoch 45, loss: 2.302681\n",
      "Epoch 46, loss: 2.301987\n",
      "Epoch 47, loss: 2.302359\n",
      "Epoch 48, loss: 2.301151\n",
      "Epoch 49, loss: 2.301573\n",
      "Epoch 50, loss: 2.302535\n",
      "Epoch 51, loss: 2.301908\n",
      "Epoch 52, loss: 2.300784\n",
      "Epoch 53, loss: 2.301531\n",
      "Epoch 54, loss: 2.302049\n",
      "Epoch 55, loss: 2.300984\n",
      "Epoch 56, loss: 2.301875\n",
      "Epoch 57, loss: 2.302054\n",
      "Epoch 58, loss: 2.301924\n",
      "Epoch 59, loss: 2.301516\n",
      "Epoch 60, loss: 2.302316\n",
      "Epoch 61, loss: 2.301111\n",
      "Epoch 62, loss: 2.303277\n",
      "Epoch 63, loss: 2.301442\n",
      "Epoch 64, loss: 2.301277\n",
      "Epoch 65, loss: 2.302840\n",
      "Epoch 66, loss: 2.301590\n",
      "Epoch 67, loss: 2.302428\n",
      "Epoch 68, loss: 2.300808\n",
      "Epoch 69, loss: 2.300832\n",
      "Epoch 70, loss: 2.302062\n",
      "Epoch 71, loss: 2.300715\n",
      "Epoch 72, loss: 2.302199\n",
      "Epoch 73, loss: 2.302919\n",
      "Epoch 74, loss: 2.300775\n",
      "Epoch 75, loss: 2.301115\n",
      "Epoch 76, loss: 2.302897\n",
      "Epoch 77, loss: 2.300990\n",
      "Epoch 78, loss: 2.301525\n",
      "Epoch 79, loss: 2.301520\n",
      "Epoch 80, loss: 2.300602\n",
      "Epoch 81, loss: 2.300362\n",
      "Epoch 82, loss: 2.302765\n",
      "Epoch 83, loss: 2.301568\n",
      "Epoch 84, loss: 2.301975\n",
      "Epoch 85, loss: 2.301107\n",
      "Epoch 86, loss: 2.301032\n",
      "Epoch 87, loss: 2.301688\n",
      "Epoch 88, loss: 2.302484\n",
      "Epoch 89, loss: 2.300937\n",
      "Epoch 90, loss: 2.301036\n",
      "Epoch 91, loss: 2.301265\n",
      "Epoch 92, loss: 2.301449\n",
      "Epoch 93, loss: 2.301634\n",
      "Epoch 94, loss: 2.300742\n",
      "Epoch 95, loss: 2.302079\n",
      "Epoch 96, loss: 2.300680\n",
      "Epoch 97, loss: 2.301750\n",
      "Epoch 98, loss: 2.300161\n",
      "Epoch 99, loss: 2.299379\n",
      "Epoch 100, loss: 2.300670\n",
      "Epoch 101, loss: 2.302397\n",
      "Epoch 102, loss: 2.301749\n",
      "Epoch 103, loss: 2.302408\n",
      "Epoch 104, loss: 2.300178\n",
      "Epoch 105, loss: 2.300388\n",
      "Epoch 106, loss: 2.301146\n",
      "Epoch 107, loss: 2.301755\n",
      "Epoch 108, loss: 2.300230\n",
      "Epoch 109, loss: 2.300976\n",
      "Epoch 110, loss: 2.301518\n",
      "Epoch 111, loss: 2.301767\n",
      "Epoch 112, loss: 2.300734\n",
      "Epoch 113, loss: 2.301121\n",
      "Epoch 114, loss: 2.299827\n",
      "Epoch 115, loss: 2.302150\n",
      "Epoch 116, loss: 2.301631\n",
      "Epoch 117, loss: 2.301192\n",
      "Epoch 118, loss: 2.300895\n",
      "Epoch 119, loss: 2.300701\n",
      "Epoch 120, loss: 2.302039\n",
      "Epoch 121, loss: 2.300968\n",
      "Epoch 122, loss: 2.301677\n",
      "Epoch 123, loss: 2.299977\n",
      "Epoch 124, loss: 2.301273\n",
      "Epoch 125, loss: 2.301731\n",
      "Epoch 126, loss: 2.301055\n",
      "Epoch 127, loss: 2.301062\n",
      "Epoch 128, loss: 2.300976\n",
      "Epoch 129, loss: 2.300974\n",
      "Epoch 130, loss: 2.300032\n",
      "Epoch 131, loss: 2.301615\n",
      "Epoch 132, loss: 2.299361\n",
      "Epoch 133, loss: 2.300248\n",
      "Epoch 134, loss: 2.299917\n",
      "Epoch 135, loss: 2.300515\n",
      "Epoch 136, loss: 2.301503\n",
      "Epoch 137, loss: 2.301901\n",
      "Epoch 138, loss: 2.299779\n",
      "Epoch 139, loss: 2.301662\n",
      "Epoch 140, loss: 2.299837\n",
      "Epoch 141, loss: 2.301135\n",
      "Epoch 142, loss: 2.301437\n",
      "Epoch 143, loss: 2.302057\n",
      "Epoch 144, loss: 2.299828\n",
      "Epoch 145, loss: 2.301088\n",
      "Epoch 146, loss: 2.302130\n",
      "Epoch 147, loss: 2.301456\n",
      "Epoch 148, loss: 2.301698\n",
      "Epoch 149, loss: 2.302656\n",
      "Epoch 150, loss: 2.300348\n",
      "Epoch 151, loss: 2.300542\n",
      "Epoch 152, loss: 2.300548\n",
      "Epoch 153, loss: 2.302069\n",
      "Epoch 154, loss: 2.300997\n",
      "Epoch 155, loss: 2.301894\n",
      "Epoch 156, loss: 2.301638\n",
      "Epoch 157, loss: 2.299616\n",
      "Epoch 158, loss: 2.299209\n",
      "Epoch 159, loss: 2.300343\n",
      "Epoch 160, loss: 2.301067\n",
      "Epoch 161, loss: 2.302289\n",
      "Epoch 162, loss: 2.300521\n",
      "Epoch 163, loss: 2.300669\n",
      "Epoch 164, loss: 2.301364\n",
      "Epoch 165, loss: 2.300391\n",
      "Epoch 166, loss: 2.300412\n",
      "Epoch 167, loss: 2.299431\n",
      "Epoch 168, loss: 2.299947\n",
      "Epoch 169, loss: 2.300907\n",
      "Epoch 170, loss: 2.300158\n",
      "Epoch 171, loss: 2.300982\n",
      "Epoch 172, loss: 2.300243\n",
      "Epoch 173, loss: 2.300557\n",
      "Epoch 174, loss: 2.300330\n",
      "Epoch 175, loss: 2.301194\n",
      "Epoch 176, loss: 2.299392\n",
      "Epoch 177, loss: 2.300747\n",
      "Epoch 178, loss: 2.301658\n",
      "Epoch 179, loss: 2.301658\n",
      "Epoch 180, loss: 2.299366\n",
      "Epoch 181, loss: 2.301155\n",
      "Epoch 182, loss: 2.301696\n",
      "Epoch 183, loss: 2.302087\n",
      "Epoch 184, loss: 2.301247\n",
      "Epoch 185, loss: 2.300351\n",
      "Epoch 186, loss: 2.299845\n",
      "Epoch 187, loss: 2.300232\n",
      "Epoch 188, loss: 2.299712\n",
      "Epoch 189, loss: 2.299924\n",
      "Epoch 190, loss: 2.301566\n",
      "Epoch 191, loss: 2.299020\n",
      "Epoch 192, loss: 2.301763\n",
      "Epoch 193, loss: 2.301201\n",
      "Epoch 194, loss: 2.299469\n",
      "Epoch 195, loss: 2.301862\n",
      "Epoch 196, loss: 2.298552\n",
      "Epoch 197, loss: 2.301140\n",
      "Epoch 198, loss: 2.300294\n",
      "Epoch 199, loss: 2.300586\n",
      "hyper-parameters: lr = 1e-05, rs = 1e-06\n",
      "best validation accuracy achieved: 0.227000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        loss_history = classifier.fit(train_X, train_y, epochs=num_epochs,\n",
    "                                      learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "            best_lr, best_rs = lr, rs\n",
    "            \n",
    "print('hyper-parameters: lr = {}, rs = {}'.format(lr, rs))\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.193000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
